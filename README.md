# ğŸ“– RAG Implementation using Groq API

## ğŸš€ Project Overview
This project is a **Retrieval-Augmented Generation (RAG)** built using the **Groq API**.  
It enables answering user queries based on knowledge extracted from **PDF documents**.  
The system demonstrates how large language models can be enhanced with external knowledge sources for more accurate and contextual responses.

---

## ğŸ› ï¸ Features
- **PDF Data Ingestion** â†’ Convert raw PDFs into structured chunks for retrieval.  
- **Chunking Process** â†’ Learned the importance of chunking for better context and semantic search.  
- **Vector Database Creation** â†’ Built a **Chroma DB** for efficient storage of embeddings.  
- **FAISS Integration** â†’ Used FAISS for **fast and scalable similarity search**.  
- **Retrieval Pipeline** â†’ Implemented query-based retrieval to fetch relevant context before response generation.  
- **Groq API Integration** â†’ Leveraged Groqâ€™s API key for LLM responses.  

---

## ğŸ“š Learning Outcomes
Through this project, I gained deeper understanding of:
- **Data Ingestion Pipeline** â€“ Converting unstructured PDFs into retrievable knowledge chunks.  
- **Data Retrieval Pipeline** â€“ Building the mechanism to fetch relevant documents.  
- **Chunking Strategy** â€“ Why smaller, overlapping text chunks improve retrieval quality.  
- **Vector Databases** â€“ How **Chroma DB** and **FAISS** are used for efficient storage & similarity search.  
- **RAG Fundamentals** â€“ Combining embeddings, retrieval, and LLMs for better results.  

---

## âš™ï¸ Tech Stack
- **Programming Language**: Python  
- **Libraries**: LangChain, FAISS, Chroma DB, PyPDF  
- **LLM Backend**: Groq API  

---

## ğŸ“Œ Future Improvements
- Enhance retrieval with **hybrid search** (semantic + keyword).  
- Add **UI for interactive querying**.  
- Optimize chunk size & overlap for different datasets.  
